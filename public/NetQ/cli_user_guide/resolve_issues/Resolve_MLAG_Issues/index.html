<!DOCTYPE html><html>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Resolve M L a G Issues | Cumulus Networks Documentation</title>

<link href="https://fonts.googleapis.com/css?family=Oxygen|Oxygen+Mono:300,400,700" rel="stylesheet">
<link rel="stylesheet" href="/normalize.min.css">

<link rel="stylesheet" href="/book.min.50a619ce4c0e99594908d162684f42218608aa6fde2bd83cca5025d59ba535e3.css">

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>

<body>
  <input type="checkbox" style="display: none" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav role="navigation">
<h2 class="book-brand">
  <a href="http://example.org/">Cumulus Networks Documentation</a>
</h2>



    


  
  
    
  
 
  <ul>
    
    <li><a href="/Cumulus-Linux/installation-management/">
    Installation Management</a> 
  <ul>
    

    
    <li>
      <a href="/Cumulus-Linux/installation-management/Adding_and_Updating_Packages/">Adding and Updating Packages</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/installation-management/Installing_a_New_Cumulus_Linux_Image/">Installing a New Cumulus Linux Image</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/installation-management/Managing_Cumulus_Linux_Disk_Images/">Managing Cumulus Linux Disk Images</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/installation-management/Upgrading_Cumulus_Linux/">Upgrading Cumulus Linux</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/installation-management/Using_Snapshots/">Using Snapshots</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/installation-management/Zero_Touch_Provisioning_-_ZTP/">Zero Touch Provisioning Z T P</a>
    </li>
    
  </ul>

    </li>
    

    
    <li>
      <a href="/Cumulus-Linux/whats-new/">What&#39;s New</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/Quick_Start_Guide/">Quick Start Guide</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/system-config/Network_Command_Line_Utility_-_NCLU/">Network Command Line Utility N C L U</a>
    </li>
    
    <li>
      <a href="/Cumulus-Linux/system-config/Setting_Date_and_Time/">Setting Date and Time</a>
    </li>
    
  </ul>
 
  <ul>
    
    <li><a href="/NetQ/UI_User_Guide/">
    U I User Guide</a> 
  <ul>
    
    <li><a href="/NetQ/UI_User_Guide/monitor_the_network/">
    Monitor the Network</a> 
  <ul>
    

    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_the_network/monitor_alarm_events/Monitor_Alarm_Events/">Monitor Alarm Events</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_the_network/monitor_informational_events/Monitor_Informational_Events/">Monitor Informational Events</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_the_network/monitor_network_health/Monitor_Network_Health/">Monitor Network Health</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_the_network/monitor_network_inventory/Monitor_Network_Inventory/">Monitor Network Inventory</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_the_network/monitor_network_protocols/Monitor_Network_Layer_Protocols/">Monitor Network Layer Protocols</a>
    </li>
    
  </ul>

    </li>
    

    
    <li>
      <a href="/NetQ/UI_User_Guide/monitor_switches/Monitor_Switches/">Monitor Switches</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/netq_ui_overview/NetQ_User_Interface_Overview/">Net Q User Interface Overview</a>
    </li>
    
    <li>
      <a href="/NetQ/UI_User_Guide/ui_preface/UI_Preface/">U I Preface</a>
    </li>
    
  </ul>

    </li>
    
    <li><a href="/NetQ/cli_user_guide/">
    Cli User Guide</a> 
  <ul>
    

    
    <li>
      <a href="/NetQ/cli_user_guide/CLI_Preface/CLI_Preface/">C L I Preface</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/automate_common_tasks/Automate_Common_and_Repetitive_Tasks/">Automate Common and Repetitive Tasks</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/command_line_overview/NetQ_Command_Line_Overview/">Net Q Command Line Overview</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_containter_environments/Monitor_Container_Environments/">Monitor Container Environments</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_data_link_layer/Monitor_Data_Link_Layer_Devices_and_Protocols/">Monitor Data Link Layer Devices and Protocols</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_linux_hosts/Monitor_Linux_Hosts/">Monitor Linux Hosts</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_network_health/Monitor_Overall_Network_Health/">Monitor Overall Network Health</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_network_layer/Monitor_Network_Protocols_and_Services/">Monitor Network Protocols and Services</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_physical_layer/Monitor_Physical_Layer_Components/">Monitor Physical Layer Components</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_switch_hardware/Monitor_Switch_Hardware_and_Software/">Monitor Switch Hardware and Software</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/monitor_virtual_overlays/Monitor_Virtual_Network_Overlays/">Monitor Virtual Network Overlays</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/resolve_issues/Resolve_Issues/">Resolve Issues</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/resolve_issues/Resolve_MLAG_Issues/" class="active">Resolve M L a G Issues</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/resolve_issues/investiage_netq_issues/Investigate_NetQ_Issues/">Investigate Net Q Issues</a>
    </li>
    
    <li>
      <a href="/NetQ/cli_user_guide/resolve_issues/methods_for_diagnosing_issues/Methods_for_Diagnosing_Network_Issues/">Methods for Diagnosing Network Issues</a>
    </li>
    
  </ul>

    </li>
    
    <li><a href="/NetQ/deployment_guide/">
    Deployment Guide</a> 
  <ul>
    

    
    <li>
      <a href="/NetQ/deployment_guide/config_optional_netq/Configuring_Optional_NetQ_Capabilities/">Configuring Optional Net Q Capabilities</a>
    </li>
    
    <li>
      <a href="/NetQ/deployment_guide/deployment_preface/Deployment_Preface/">Deployment Preface</a>
    </li>
    
    <li>
      <a href="/NetQ/deployment_guide/install_netq/Install_NetQ/">Install Net Q</a>
    </li>
    
    <li>
      <a href="/NetQ/deployment_guide/maintain_netq/Maintain_NetQ/">Maintain Net Q</a>
    </li>
    
    <li>
      <a href="/NetQ/deployment_guide/netq_primer/Cumulus_NetQ_Primer/">Cumulus Net Q Primer</a>
    </li>
    
  </ul>

    </li>
    

    
    <li>
      <a href="/NetQ/Release_Notes/Cumulus_NetQ_2.0_Release_Notes/">Cumulus Net Q 2.0 Release Notes</a>
    </li>
    
    <li>
      <a href="/NetQ/getting_started/Getting_Started_with_the_Cumulus_NetQ_Appliance/">Getting Started With the Cumulus Net Q Appliance</a>
    </li>
    
    <li>
      <a href="/NetQ/software_installation_guide/Cumulus_NetQ_Appliance_Software_Installation_Guide/">Cumulus Net Q Appliance Software Installation Guide</a>
    </li>
    
  </ul>








</nav>

    </aside>

    <div class="book-page">
      <header class="align-center justify-between book-header">
  <label for="menu-control">
    <img src="/svg/menu.svg" />
  </label>
  <strong>Resolve M L a G Issues</strong>
</header>

      
<article class="markdown">

<h1 id="resolve-mlag-issues">Resolve MLAG Issues</h1>

<p>This topic outlines a few scenarios that illustrate how you use NetQ to
troubleshoot
<a href="https://docs.cumulusnetworks.com/display/DOCS/Multi-Chassis+Link+Aggregation+-+MLAG">MLAG</a>
on Cumulus Linux switches. Each starts with a log message that indicates
the current MLAG state.</p>

<p>NetQ can monitor many aspects of an MLAG configuration, including:</p>

<ul>
<li>Verifying the current state of all nodes</li>
<li>Verifying the dual connectivity state</li>
<li>Checking that the peer link is part of the bridge</li>
<li>Verifying whether MLAG bonds are not bridge members</li>
<li>Verifying whether the VXLAN interface is not a bridge member</li>
<li>Checking for remote-side service failures caused by <code>systemctl</code></li>
<li>Checking for VLAN-VNI mapping mismatches</li>
<li>Checking for layer 3 MTU mismatches on peerlink subinterfaces</li>
<li>Checking for VXLAN active-active address inconsistencies</li>
<li>Verifying that STP priorities are the same across both peers</li>
</ul>

<h2 id="contents">Contents</h2>

<p><img src="images/icons/grey_arrow_down.png" alt="" />{.expand-control-image}This topic
describes&hellip;</p>

<ul>
<li><a href="#ResolveMLAGIssues-Scenario:AllNodesAreUp">Scenario: All Nodes Are
Up</a></li>
<li><a href="#ResolveMLAGIssues-Scenario:Dual-connectedBondIsDown">Scenario: Dual-connected Bond Is
Down</a></li>
<li><a href="#ResolveMLAGIssues-Scenario:VXLANActive-activeDeviceorInterfaceIsDown">Scenario: VXLAN Active-active Device or Interface Is
Down</a></li>
<li><a href="#ResolveMLAGIssues-Scenario:Remote-sideclagdStoppedbysystemctlCommand">Scenario: Remote-side clagd Stopped by systemctl
Command</a></li>
</ul>

<h2 id="scenario-all-nodes-are-up">Scenario: All Nodes Are Up</h2>

<p>When the MLAG configuration is running smoothly, NetQ sends out a
message that all nodes are up:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">2017-05-22T23:13:09.683429+00:00 noc-pr netq-notifier[5501]: INFO: CLAG: All nodes are up</code></pre></div>
<p>Running <code>netq show clag</code> confirms this:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq show clag
Matching clag records:
Hostname          Peer              SysMac             State      Backup #Bond #Dual Last Changed
                                                                         s
----------------- ----------------- ------------------ ---------- ------ ----- ----- -------------------------
spine01(P)        spine02           00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:49 2019
spine02           spine01(P)        00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:53 2019
leaf01(P)         leaf02            44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:15 2019
leaf02            leaf01(P)         44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:20 2019
leaf03(P)         leaf04            44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:26 2019
leaf04            leaf03(P)         44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:30 2019</code></pre></div>
<p>You can also verify a specific node is up:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq spine01 show clag
Matching clag records:
Hostname          Peer              SysMac             State      Backup #Bond #Dual Last Changed
                                                                         s
----------------- ----------------- ------------------ ---------- ------ ----- ----- -------------------------
spine01(P)        spine02           00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:49 2019</code></pre></div>
<p>Similarly, checking the MLAG state with NetQ also confirms this:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag
Checked Nodes: 6, Failed Nodes: 0</code></pre></div>
<p>When you are logged directly into a switch, you can run <code>clagctl</code> to get
the state:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:/var/log# sudo clagctl

The peer is alive
Peer Priority, ID, and Role: 4096 00:02:00:00:00:4e primary
Our Priority, ID, and Role: 8192 44:38:39:00:a5:38 secondary
Peer Interface and IP: peerlink-3.4094 169.254.0.9 
VxLAN Anycast IP: 36.0.0.20
Backup IP: 27.0.0.20 (active)
System MAC: 44:38:39:ff:ff:01

CLAG Interfaces
Our Interface    Peer Interface   CLAG Id Conflicts            Proto-Down Reason
---------------- ---------------- ------- -------------------- -----------------
vx-38            vx-38            -       -                    - 
vx-33            vx-33            -       -                    - 
hostbond4        hostbond4        1       -                    - 
hostbond5        hostbond5        2       -                    - 
vx-37            vx-37            -       -                    - 
vx-36            vx-36            -       -                    - 
vx-35            vx-35            -       -                    - 
vx-34            vx-34            -       -                    -</code></pre></div>
<h2 id="scenario-dual-connected-bond-is-down">Scenario: Dual-connected Bond Is Down</h2>

<p>When dual connectivity is lost in an MLAG configuration, you receive
messages from NetQ similar to the following:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">2017-05-22T23:14:40.290918+00:00 noc-pr netq-notifier[5501]: WARNING: LINK: 1 link(s) are down. They are: spine01 hostbond5
2017-05-22T23:14:53.081480+00:00 noc-pr netq-notifier[5501]: WARNING: CLAG: 1 node(s) have failures. They are: spine01
2017-05-22T23:14:58.161267+00:00 noc-pr netq-notifier[5501]: WARNING: CLAG: 2 node(s) have failures. They are: spine01, leaf01</code></pre></div>
<p>To begin your investigation, show the status of the <code>clagd</code> service:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq spine01 show services clagd

Matching services records:
Hostname          Service              PID   VRF             Enabled Active Monitored Status           Uptime                    Last Changed
----------------- -------------------- ----- --------------- ------- ------ --------- ---------------- ------------------------- -------------------------
spine01           clagd                2678  default         yes     yes    yes       ok               23h:57m:16s               Thu Feb  7 18:30:49 2019</code></pre></div>
<p>Checking the MLAG status provides the reason for the failure:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag
Checked Nodes: 6, Warning Nodes: 2
Node             Reason
---------------- --------------------------------------------------------------------------
spine01          Link Down: hostbond5
leaf01           Singly Attached Bonds: hostbond5</code></pre></div>
<p>You can retrieve the output in JSON format for export to another tool:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag json
{
    &#34;warningNodes&#34;: [
        { 
            &#34;node&#34;: &#34;spine01&#34;, 
            &#34;reason&#34;: &#34;Link Down: hostbond5&#34; 
        }
        ,
        { 
            &#34;node&#34;: &#34;lea01&#34;, 
            &#34;reason&#34;: &#34;Singly Attached Bonds: hostbond5&#34; 
        }
    ], 
    &#34;failedNodes&#34;:[
    ],
    &#34;summary&#34;:{
        &#34;checkedNodeCount&#34;:6,
        &#34;failedNodeCount&#34;:0,
        &#34;warningNodeCount&#34;:2
    }
}</code></pre></div>
<p>After you fix the issue, you can show the MLAG state to see if all the
nodes are up. The notifications from NetQ indicate all nodes are UP, and
the <code>netq check</code> flag also indicates there are no failures.</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq show clag

Matching clag records:
Hostname          Peer              SysMac             State      Backup #Bond #Dual Last Changed
                                                                         s
----------------- ----------------- ------------------ ---------- ------ ----- ----- -------------------------
spine01(P)        spine02           00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:49 2019
spine02           spine01(P)        00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:53 2019
leaf01(P)         leaf02            44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:15 2019
leaf02            leaf01(P)         44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:20 2019
leaf03(P)         leaf04            44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:26 2019
leaf04            leaf03(P)         44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:30 2019</code></pre></div>
<p>When you are logged directly into a switch, you can run <code>clagctl</code> to get
the state:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:/var/log# sudo clagctl

The peer is alive
Peer Priority, ID, and Role: 4096 00:02:00:00:00:4e primary
Our Priority, ID, and Role: 8192 44:38:39:00:a5:38 secondary
Peer Interface and IP: peerlink-3.4094 169.254.0.9 
VxLAN Anycast IP: 36.0.0.20
Backup IP: 27.0.0.20 (active)
System MAC: 44:38:39:ff:ff:01

CLAG Interfaces
Our Interface    Peer Interface   CLAG Id Conflicts            Proto-Down Reason
---------------- ---------------- ------- -------------------- -----------------
vx-38            vx-38            -       -                    - 
vx-33            vx-33            -       -                    - 
hostbond4        hostbond4        1       -                    - 
hostbond5        -                2       -                    - 
vx-37            vx-37            -       -                    - 
vx-36            vx-36            -       -                    - 
vx-35            vx-35            -       -                    - 
vx-34            vx-34            -       -                    - </code></pre></div>
<h2 id="scenario-vxlan-active-active-device-or-interface-is-down">Scenario: VXLAN Active-active Device or Interface Is Down</h2>

<p>When a VXLAN active-active device or interface in an MLAG configuration
is down, log messages also include VXLAN and LNV checks.</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">2017-05-22T23:16:51.517522+00:00 noc-pr netq-notifier[5501]: WARNING: VXLAN: 2 node(s) have failures. They are: spine01, leaf01
2017-05-22T23:16:51.525403+00:00 noc-pr netq-notifier[5501]: WARNING: LINK: 2 link(s) are down. They are: leaf01 vx-37, spine01 vx-37
2017-05-22T23:16:54.194681+00:00 noc-pr netq-notifier[5501]: WARNING: LNV: 1 node(s) have failures. They are: leaf02
2017-05-22T23:16:59.448755+00:00 noc-pr netq-notifier[5501]: WARNING: LNV: 3 node(s) have failures. They are: leaf01, leaf03, leaf04
2017-05-22T23:17:04.703044+00:00 noc-pr netq-notifier[5501]: WARNING: CLAG: 2 node(s) have failures. They are: spine01, leaf01</code></pre></div>
<p>To begin your investigation, show the status of the <code>clagd</code> service:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq spine01 show services clagd

Matching services records:
Hostname          Service              PID   VRF             Enabled Active Monitored Status           Uptime                    Last Changed
----------------- -------------------- ----- --------------- ------- ------ --------- ---------------- ------------------------- -------------------------
spine01           clagd                2678  default         yes     yes    yes       error            23h:57m:16s               Thu Feb  7 18:30:49 2019</code></pre></div>
<p>Checking the MLAG status provides the reason for the failure:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag
Checked Nodes: 6, Warning Nodes: 2, Failed Nodes: 2 
Node             Reason
---------------- --------------------------------------------------------------------------
spine01          Protodown Bonds: vx-37:vxlan-single 
leaf01           Protodown Bonds: vx-37:vxlan-single </code></pre></div>
<p>You can retrieve the output in JSON format for export to another tool:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag json
{
    &#34;failedNodes&#34;: [
        { 
            &#34;node&#34;: &#34;spine01&#34;, 
            &#34;reason&#34;: &#34;Protodown Bonds: vx-37:vxlan-single&#34; 
        }
        ,
        { 
            &#34;node&#34;: &#34;leaf01&#34;, 
            &#34;reason&#34;: &#34;Protodown Bonds: vx-37:vxlan-single&#34; 
        }
    ], 
    &#34;summary&#34;:{ 
            &#34;checkedNodeCount&#34;: 6, 
            &#34;failedNodeCount&#34;: 2, 
            &#34;warningNodeCount&#34;: 2 
    }
}</code></pre></div>
<p>After you fix the issue, you can show the MLAG state to see if all the
nodes are up:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq show clag
Matching clag session records are:
Hostname          Peer              SysMac             State      Backup #Bond #Dual Last Changed
                                                                         s
----------------- ----------------- ------------------ ---------- ------ ----- ----- -------------------------
spine01(P)        spine02           00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:49 2019
spine02           spine01(P)        00:01:01:10:00:01  up         up     24    24    Thu Feb  7 18:30:53 2019
leaf01(P)         leaf02            44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:15 2019
leaf02            leaf01(P)         44:38:39:ff:ff:01  up         up     12    12    Thu Feb  7 18:31:20 2019
leaf03(P)         leaf04            44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:26 2019
leaf04            leaf03(P)         44:38:39:ff:ff:02  up         up     12    12    Thu Feb  7 18:31:30 2019</code></pre></div>
<p>When you are logged directly into a switch, you can run <code>clagctl</code> to get
the state:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:/var/log# sudo clagctl

The peer is alive
Peer Priority, ID, and Role: 4096 00:02:00:00:00:4e primary
Our Priority, ID, and Role: 8192 44:38:39:00:a5:38 secondary
Peer Interface and IP: peerlink-3.4094 169.254.0.9 
VxLAN Anycast IP: 36.0.0.20
Backup IP: 27.0.0.20 (active)
System MAC: 44:38:39:ff:ff:01

CLAG Interfaces
Our Interface    Peer Interface   CLAG Id Conflicts            Proto-Down Reason
---------------- ---------------- ------- -------------------- -----------------
vx-38            vx-38            -       -                    - 
vx-33            vx-33            -       -                    - 
hostbond4        hostbond4        1       -                    - 
hostbond5        hostbond5        2       -                    - 
vx-37            -                -       -                    vxlan-single 
vx-36            vx-36            -       -                    - 
vx-35            vx-35            -       -                    - 
vx-34            vx-34            -       -                    - </code></pre></div>
<h2 id="scenario-remote-side-clagd-stopped-by-systemctl-command">Scenario: Remote-side clagd Stopped by systemctl Command</h2>

<p>In the event the <code>clagd</code> service is stopped via the <code>systemctl</code> command,
NetQ Notifier sends messages similar to the following:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">2017-05-22T23:51:19.539033+00:00 noc-pr netq-notifier[5501]: WARNING: VXLAN: 1 node(s) have failures. They are: leaf01
2017-05-22T23:51:19.622379+00:00 noc-pr netq-notifier[5501]: WARNING: LINK: 2 link(s) flapped and are down. They are: leaf01 hostbond5, leaf01 hostbond4
2017-05-22T23:51:19.622922+00:00 noc-pr netq-notifier[5501]: WARNING: LINK: 23 link(s) are down. They are: leaf01 VlanA-1-104-v0, leaf01 VlanA-1-101-v0, leaf01 VlanA-1, leaf01 vx-33, leaf01 vx-36, leaf01 vx-37, leaf01 vx-34, leaf01 vx-35, leaf01 swp7, leaf01 VlanA-1-102-v0, leaf01 VlanA-1-103-v0, leaf01 VlanA-1-100-v0, leaf01 VlanA-1-106-v0, leaf01 swp8, leaf01 VlanA-1.106, leaf01 VlanA-1.105, leaf01 VlanA-1.104, leaf01 VlanA-1.103, leaf01 VlanA-1.102, leaf01 VlanA-1.101, leaf01 VlanA-1.100, leaf01 VlanA-1-105-v0, leaf01 vx-38
2017-05-22T23:51:27.696572+00:00 noc-pr netq-notifier[5501]: INFO: LINK: 15 link(s) are up. They are: leaf01 VlanA-1.106, leaf01 VlanA-1-104-v0, leaf01 VlanA-1.104, leaf01 VlanA-1.103, leaf01 VlanA-1.101, leaf01 VlanA-1-100-v0, leaf01 VlanA-1.100, leaf01 VlanA-1.102, leaf01 VlanA-1-101-v0, leaf01 VlanA-1-102-v0, leaf01 VlanA-1.105, leaf01 VlanA-1-103-v0, leaf01 VlanA-1-106-v0, leaf01 VlanA-1, leaf01 VlanA-1-105-v0
2017-05-22T23:51:30.863789+00:00 noc-pr netq-notifier[5501]: WARNING: LNV: 1 node(s) have failures. They are: leaf01
2017-05-22T23:51:36.156708+00:00 noc-pr netq-notifier[5501]: WARNING: CLAG: 2 node(s) have failures. They are: spine01, leaf01
2017-05-22T23:51:36.183638+00:00 noc-pr netq-notifier[5501]: WARNING: LNV: 2 node(s) have failures. They are: spine02, leaf01
2017-05-22T23:51:41.444670+00:00 noc-pr netq-notifier[5501]: WARNING: LNV: 1 node(s) have failures. They are: leaf01</code></pre></div>
<p>Showing the MLAG state reveals which nodes are down:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq show clag
Matching CLAG session records are:
Node             Peer             SysMac            State Backup #Bonds #Dual Last Changed
---------------- ---------------- ----------------- ----- ------ ------ ----- -------------------------
spine01(P)       spine02           00:01:01:10:00:01 up   up     9      9     Thu Feb  7 18:30:53 2019
spine02          spine01(P)        00:01:01:10:00:01 up   up     9      9     Thu Feb  7 18:31:04 2019
leaf01                             44:38:39:ff:ff:01 down n/a    0      0     Thu Feb  7 18:31:13 2019
leaf03(P)        leaf04            44:38:39:ff:ff:02 up   up     8      8     Thu Feb  7 18:31:19 2019
leaf04           leaf03(P)         44:38:39:ff:ff:02 up   up     8      8     Thu Feb  7 18:31:25 2019</code></pre></div>
<p>Checking the MLAG status provides the reason for the failure:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag
Checked Nodes: 6, Warning Nodes: 1, Failed Nodes: 2 
Node             Reason
---------------- --------------------------------------------------------------------------
spine01          Peer Connectivity failed 
leaf01           Peer Connectivity failed </code></pre></div>
<p>You can retrieve the output in JSON format for export to another tool:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ netq check clag json
{
    &#34;failedNodes&#34;: [
        { 
            &#34;node&#34;: &#34;spine01&#34;, 
            &#34;reason&#34;: &#34;Peer Connectivity failed&#34; 
        }
        ,
        { 
            &#34;node&#34;: &#34;leaf01&#34;, 
            &#34;reason&#34;: &#34;Peer Connectivity failed&#34; 
        }
    ], 
    &#34;summary&#34;:{ 
        &#34;checkedNodeCount&#34;: 6, 
        &#34;failedNodeCount&#34;: 2, 
        &#34;warningNodeCount&#34;: 1 
    }
}</code></pre></div>
<p>When you are logged directly into a switch, you can run <code>clagctl</code> to get
the state:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">cumulus@switch:~$ sudo clagctl

The peer is not alive
Our Priority, ID, and Role: 8192 44:38:39:00:a5:38 primary
Peer Interface and IP: peerlink-3.4094 169.254.0.9 
VxLAN Anycast IP: 36.0.0.20
Backup IP: 27.0.0.20 (inactive)
System MAC: 44:38:39:ff:ff:01

CLAG Interfaces
Our Interface    Peer Interface   CLAG Id Conflicts            Proto-Down Reason
---------------- ---------------- ------- -------------------- -----------------
vx-38            -                -       -                    - 
vx-33            -                -       -                    - 
hostbond4        -                1       -                    - 
hostbond5        -                2       -                    - 
vx-37            -                -       -                    -
vx-36            -                -       -                    -
vx-35            -                -       -                    -
vx-34            -                -       -                    -</code></pre></div></article>

      

    </div>

    
  
  
  <aside class="book-toc fixed">
    <nav id="TableOfContents">
<ul>
<li><a href="#resolve-mlag-issues">Resolve MLAG Issues</a>
<ul>
<li><a href="#contents">Contents</a></li>
<li><a href="#scenario-all-nodes-are-up">Scenario: All Nodes Are Up</a></li>
<li><a href="#scenario-dual-connected-bond-is-down">Scenario: Dual-connected Bond Is Down</a></li>
<li><a href="#scenario-vxlan-active-active-device-or-interface-is-down">Scenario: VXLAN Active-active Device or Interface Is Down</a></li>
<li><a href="#scenario-remote-side-clagd-stopped-by-systemctl-command">Scenario: Remote-side clagd Stopped by systemctl Command</a></li>
</ul></li>
</ul>
</nav>
  </aside>



  </main>

  
  
</body>

</html>
