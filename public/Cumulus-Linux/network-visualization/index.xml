<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cumulus Networks Documentation</title>
    <link>http://example.org/Cumulus-Linux/network-visualization/</link>
    <description>Recent content on Cumulus Networks Documentation</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="http://example.org/Cumulus-Linux/network-visualization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Eng_Update_of_VXLAN_Hyperloop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Eng_Update_of_VXLAN_Hyperloop/</guid>
      <description>Eng Update of VXLAN Hyperloop This chapter describes how to configure VXLAN gateways using a loopback cable (called a hyperloop) on non-RIOT (VXLAN routing) capable ASICs running Cumulus Linux. The Broadcom Trident II and Tomahawk ASICs have a limitation where a layer 2 bridge that contains a VXLAN interface cannot also have an IP address assigned to it. This is an expected limitation with this ASIC because of the ordering of the decapsulation.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Ethernet_Virtual_Private_Network_-_EVPN/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Ethernet_Virtual_Private_Network_-_EVPN/</guid>
      <description>Ethernet Virtual Private Network - EVPN VXLAN is the de facto technology for implementing network virtualization in the data center, enabling layer 2 segments to be extended over an IP core (the underlay). The initial definition of VXLAN (RFC 7348) did not include any control plane and relied on a flood-and-learn approach for MAC address learning. An alternate deployment model was to use a controller or a technology such as Lightweight Network Virtualization (LNV) in Cumulus Linux.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Hybrid_Cloud_Connectivity_with_QinQ_and_VXLANs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Hybrid_Cloud_Connectivity_with_QinQ_and_VXLANs/</guid>
      <description>Hybrid Cloud Connectivity with QinQ and VXLANs QinQ is an amendment to the IEEE 802.1Q specification that provides the capability for multiple VLAN tags to be inserted into a single Ethernet frame. The primary use case for QinQ with VXLAN is where a service provider who offers multi-tenant layer 2 connectivity between different customers’ data centers (private clouds) may also need to connect those data centers to public cloud providers.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/LNV_Full_Example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/LNV_Full_Example/</guid>
      <description>LNV Full Example Lightweight Network Virtualization (LNV) is a technique for deploying VXLANs without a central controller on bare metal switches. This a full example complete with diagram. Refer to the Lightweight Network Virtualization chapter for more detailed information. This full example uses the recommended way of deploying LNV, which is to use anycast to load balance the service nodes.
LNV is a lightweight controller option. Contact Cumulus Networks with your scale requirements so we can make sure this is the right fit for you.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Lightweight_Network_Virtualization_Overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Lightweight_Network_Virtualization_Overview/</guid>
      <description>Lightweight Network Virtualization Overview Lightweight Network Virtualization (LNV) is a technique for deploying VXLANs without a central controller on bare metal switches. This solution requires no external controller or software suite; it runs the VXLAN service and registration daemons on Cumulus Linux itself. The data path between bridge entities is established on top of a layer 3 fabric by means of a simple service node coupled with traditional MAC address learning.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Static_VXLAN_Configurations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Static_VXLAN_Configurations/</guid>
      <description> Static VXLAN Configurations  Static VXLAN Tunnels Static MAC Bindings with VXLAN  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Routing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Routing/</guid>
      <description>VXLAN Routing VXLAN routing, sometimes referred to as inter-VXLAN routing, provides IP routing between VXLAN VNIs in overlay networks. The routing of traffic is based on the inner header or the overlay tenant IP address.
Because VXLAN routing is fundamentally routing, it is most commonly deployed with a control plane, such as Ethernet Virtual Private Network (EVPN). You can set up static routing too, either with or without the Cumulus Lightweight Network Virtualization (LNV) for MAC distribution and BUM handling.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Scale/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Scale/</guid>
      <description>VXLAN Scale On Broadcom Trident II and Tomahawk switches running Cumulus Linux, there is a limit to the number of VXLANs you can configure simultaneously. The limit most often given is 2000 VXLANs, but you might want to get more specific and know exactly the limit for your specific design.
While this limitation does apply to Trident II+, Trident3, or Maverick ASICs, Cumulus Linux supports the same number of VXLANs on these ASICs as it does for Trident II or Tomahawk ASICs.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Tunnel_DSCP_Operations__DRAFT/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/VXLAN_Tunnel_DSCP_Operations__DRAFT/</guid>
      <description>VXLAN Tunnel DSCP Operations — DRAFT The following configuration options control DSCP operations during VXLAN encapsulation and decapsulation. These operations work for applications that require end-to-end quality of service, such as RDMA over Converged Ethernet.
On Mellanox switches, you can use these options to propagate explicit congestion notification (ECN) between the underlay and overlay. Because they are based on RFC-6040, which describes how to construct the IP header of an ECN field on both ingress to and egress from an IP-in-IP tunnel, this feature is always enabled.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/Virtualization_Integrations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/Virtualization_Integrations/</guid>
      <description> Virtualization Integrations Cumulus Linux integrates with a number of VXLAN controller-based virtualization solutions.
 Integrating Hardware VTEPs with Midokura MidoNet and OpenStack Integrating Hardware VTEPs with VMware NSX-V Integrating Hardware VTEPs with VMware NSX-MH OVSDB Server High Availability  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/static_vxlan_configurations/static_mac_bindings/Static_MAC_Bindings_with_VXLAN/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/static_vxlan_configurations/static_mac_bindings/Static_MAC_Bindings_with_VXLAN/</guid>
      <description>Static MAC Bindings with VXLAN Cumulus Linux includes native Linux VXLAN kernel support.
Contents {.expand-control-image}This topic describes &amp;hellip;
 Requirements Example VXLAN Configuration Configure the Static MAC Bindings VXLAN Troubleshooting  Requirements A VXLAN configuration requires a Broadcom switch with the Tomahawk, Trident II+, or Trident II ASIC running Cumulus Linux 2.0 or later, or a Mellanox switch with the Spectrum ASIC running Cumulus Linux 3.2.0 or later.
For a basic VXLAN configuration, make sure that:</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/static_vxlan_configurations/static_vxlan_tunnels/Static_VXLAN_Tunnels/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/static_vxlan_configurations/static_vxlan_tunnels/Static_VXLAN_Tunnels/</guid>
      <description>Static VXLAN Tunnels In VXLAN-based networks, there are a range of complexities and challenges in determining the destination *virtual tunnel endpoints* (VTEPs) for any given VXLAN. At scale, various solutions, including Lightweight Network Virtualization (LNV), controller-based options like Midokura MidoNet or VMware NSX and even new standards like EVPN are attempts to address these complexities, however do retain their own complexities.
Enter static VXLAN tunnels, which simply serve to connect two VTEPs in a given environment.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_hardware_vteps_vmware_nsx-mh/Integrating_Hardware_VTEPs_with_VMware_NSX-MH/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_hardware_vteps_vmware_nsx-mh/Integrating_Hardware_VTEPs_with_VMware_NSX-MH/</guid>
      <description>Integrating Hardware VTEPs with VMware NSX-MH Switches running Cumulus Linux can integrate with VMware NSX Multi-Hypervisor (MH) to act as hardware VTEP gateways. The VMware NSX-MH controller provides consistent provisioning across virtual and physical server infrastructures.
{width=&amp;ldquo;500&amp;rdquo;}
Cumulus Linux also supports integration with VMware NSX in high availability mode. Refer to OVSDB Server High Availability.
Contents {.expand-control-image}This topic describes &amp;hellip;
 Getting Started Configure the Switch for NSX-MH Integration  Start the openvswitch-vtep Service Configure the NSX-MH Integration Using the Configuration Script Configure the NSX-MH Integration Manually  Provision VMware NSX-V  Configure the Switch as a VTEP Gateway  Configure the Transport and Logical Layers  Configure the Transport Layer Configure the Logical Layer Define Logical Switch Ports  Verify the VXLAN Configuration  Getting Started Before you integrate VXLANs with NSX-MH, make sure you have a layer 2 gateway; a Broadcom Tomahawk, Trident II+, Trident II, Maverick, or Mellanox Spectrum switch running Cumulus Linux.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_vteps_midonet_openstack/Integrating_Hardware_VTEPs_with_Midokura_MidoNet_and_OpenStack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_vteps_midonet_openstack/Integrating_Hardware_VTEPs_with_Midokura_MidoNet_and_OpenStack/</guid>
      <description>Integrating Hardware VTEPs with Midokura MidoNet and OpenStack Cumulus Linux seamlessly integrates with the MidoNet OpenStack infrastructure, where the switches provide the VTEP gateway for terminating VXLAN tunnels from within MidoNet. MidoNet connects to the OVSDB server running on the Cumulus Linux switch, and exchanges information about the VTEPs and MAC addresses associated with the OpenStack Neutron networks. This provides seamless Ethernet connectivity between virtual and physical server infrastructures.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_vteps_vmware_nsx-v/Integrating_Hardware_VTEPs_with_VMware_NSX-V/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/integrating_vteps_vmware_nsx-v/Integrating_Hardware_VTEPs_with_VMware_NSX-V/</guid>
      <description>Integrating Hardware VTEPs with VMware NSX-V Switches running Cumulus Linux can integrate with VMware NSX-V to act as hardware VTEP gateways. The VMware NSX-V controller provides consistent provisioning across virtual and physical server infrastructures.
{width=&amp;ldquo;500&amp;rdquo;}
Cumulus Linux also supports integration with VMware NSX in high availability mode. Refer to OVSDB Server High Availability.
Contents {.expand-control-image}This topic describes &amp;hellip;
 Getting Started Configure the Switch for NSX-V Integration  Start the openvswitch-vtep Service Configure the NSX-V Integration Using the Configuration Script Configure the NSX-V Integration Manually  Provision VMware NSX-V  Configure the Switch as a VTEP Gateway  Configure the Transport and Logical Layers  Configure the Transport Layer Configure the Logical Layer Define Logical Switch Ports  Verify the VXLAN Configuration  Getting Started Before you integrate VXLANs with NSX-V, make sure you have a layer 2 gateway; a Broadcom Tomahawk, Trident II+, Trident II, Maverick or Mellanox Spectrum switch running Cumulus Linux.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/ovsdb_server_high_availability/OVSDB_Server_High_Availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://example.org/Cumulus-Linux/network-visualization/virtualization_integrations/ovsdb_server_high_availability/OVSDB_Server_High_Availability/</guid>
      <description>OVSDB Server High Availability Early Access Feature
OVSDB server high availability is an early access feature in Cumulus Linux 3.7.
Cumulus Linux supports integration with VMware NSX in both standalone mode and OVSDB server high availability mode (where the data plane is running in active-active mode). For information about VMware NSX in standalone mode and for a description of the components that work together to integrate VMware NSX and Cumulus Linux, see Integrating Hardware VTEPs with VMware NSX-MH or Integrating Hardware VTEPs with VMware NSX-V.</description>
    </item>
    
  </channel>
</rss>